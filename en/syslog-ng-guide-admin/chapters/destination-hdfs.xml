<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section
 [  <!ENTITY % entities SYSTEM "../../shared/syslog-ng-entities.ent">
 %entities;]>
<section xml:id="configuring-destinations-hdfs" xmlns="http://docbook.org/ns/docbook" version="5.0">
    <title><parameter>hdfs</parameter>: Storing messages on the Hadoop Distributed File System (HDFS)</title>
    <indexterm> <primary>destination drivers</primary> <secondary><parameter>java()</parameter> driver</secondary> </indexterm>
    <indexterm> <primary>destination drivers</primary> <secondary><parameter>hdfs</parameter></secondary> </indexterm>
    <para>Starting with version <phrase condition="pe">5.3</phrase><phrase condition="ose">3.7</phrase>, &abbrev; can send plain-text log files to the <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="http://hadoop.apache.org/">Hadoop Distributed File System (HDFS)</link>, allowing you to store your log data on a distributed, scalable file system. This is especially useful if you have huge amounts of log messages that would be difficult to store otherwise, or if you want to process your messages using Hadoop tools (for example, Apache Pig).</para>
    <para>For more information about the benefits of using syslog-ng as a data collection, processing, and filtering tool in a Hadoop environment, see the blog post <link xmlns:xlink="http://www.w3.org/1999/xlink" xlink:href="https://syslog-ng.com/blog/filling-your-data-lake-with-log-messages-the-syslog-ng-hadoop-hdfs-destination/">Filling your data lake with log messages: the syslog-ng Hadoop (HDFS) destination</link>.</para>
    <para>Note the following limitations when using the &abbrev; <parameter>hdfs</parameter> destination:</para>
    <itemizedlist>
        <xi:include href="../../shared/chunk/listitem-java-supported-platforms.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        <listitem>
            <para>Since &abbrev; uses the official Java HDFS client, the <parameter>hdfs</parameter> destination has significant memory usage (about 400MB).</para>
        </listitem>
        <listitem>
            <xi:include href="../../shared/chunk/para-hdfs-flush.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </listitem>
        <xi:include href="../../shared/chunk/listitem-java-internal-messages.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    </itemizedlist>
    <formalpara>
        <title>Declaration:</title>
        <para/>
    </formalpara>
    <synopsis>@module mod-java
@include "scl.conf"

hdfs(
    client-lib-dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:&lt;path-to-preinstalled-hadoop-libraries&gt;")
    hdfs-uri("hdfs://NameNode:8020")
    hdfs-file("&lt;path-to-logfile&gt;")
);</synopsis>
    <example xml:id="example-destination-hdfs">
        <title>Storing logfiles on HDFS</title>
        <para>The following example defines an <parameter>hdfs</parameter> destination using only the required parameters.</para>
        <synopsis>@module mod-java
@include "scl.conf"

destination d_hdfs {
    hdfs(
        client-lib-dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:/opt/hadoop/libs")
        hdfs-uri("hdfs://10.140.32.80:8020")
        hdfs-file("/user/log/logfile.txt")
    );
};
</synopsis>
    </example>
    <itemizedlist>
        <listitem>
            <para>To install the software required for the <parameter>hdfs</parameter> destination, see <xref linkend="destination-hdfs-prerequisites"/>.</para>
        </listitem>
        <listitem>
            <para>For details on how the <parameter>hdfs</parameter> destination works, see <xref linkend="destination-hdfs-interaction"/>.</para>
        </listitem>
        <listitem>
            <para>For details on using MapR-FS, see <xref linkend="destination-hdfs-maprfs"/>.</para>
        </listitem>
        <listitem>
            <para>For details on using Kerberos authentication, see <xref linkend="destination-hdfs-kerberos-authentication"/>.</para>
        </listitem>
        <listitem>
            <para>For the list of options, see <xref linkend="reference-destination-hdfs"/>.</para>
        </listitem>
    </itemizedlist>
    <para condition="ose">The <parameter>hdfs()</parameter> driver is actually a reusable configuration snippet configured to receive log messages using the Java language-binding of &abbrev;. For details on using or writing such configuration snippets, see <xref linkend="config-blocks"/>. You can find the source of the hdfs configuration snippet on <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="https://github.com/balabit/syslog-ng/blob/master/scl/hdfs/plugin.conf">GitHub</link>. For details on extending &abbrev; in Java, see the <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="https://syslog-ng.gitbooks.io/getting-started/content/chapters/chapter_5/section_2.html">Getting started with syslog-ng development</link> guide.</para>
    <procedure xml:id="destination-hdfs-prerequisites">
        <title>Prerequisites</title>
        <para>To send messages from &abbrev; to HDFS, complete the following steps.</para>
        <formalpara>
            <title>Steps:</title>
            <para/>
        </formalpara>
        <step>
            <xi:include href="../../shared/chunk/para-java-requirements.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </step>
        <step>
            <para>Download the Hadoop Distributed File System (HDFS) libraries (version 2.x) from <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="http://hadoop.apache.org/releases.html">http://hadoop.apache.org/releases.html</link>.</para>
        </step>
        <step>
            <para>Extract the HDFS libraries into a temporary directory, then collect the various <filename>.jar</filename> files into a single directory (for example, <filename>/opt/hadoop/lib/</filename>) where &abbrev; can access them. You must specify this directory in the &abbrev; configuration file. The files are located in the various <filename>lib</filename> directories under the <filename>share/</filename> directory of the Hadoop release package. (For example, in Hadoop 2.7, required files are <filename>common/hadoop-common-2.7.0.jar</filename>, <filename>common/libs/*.jar</filename>, <filename>hdfs/hadoop-hdfs-2.7.0.jar</filename>, <filename>hdfs/lib/*</filename>, but this may change between Hadoop releases, so it is easier to copy every <filename>.jar</filename> file into a single directory.</para>
        </step>
        <!-- The java home must be given at installation time for the PE installer. FIXME: include it in the PE installer description!  -->
    </procedure>
    <procedure xml:id="destination-hdfs-interaction">
        <title>How &abbrev; interacts with HDFS</title>
        <para>The &abbrev; application sends the log messages to the official HDFS client library, which forwards the data to the HDFS nodes. The way &abbrev; interacts with HDFS is described in the following steps.</para>
        <step>
            <para>After &abbrev; is started and the first message arrives to the <parameter>hdfs</parameter> destination, the <parameter>hdfs</parameter> destination tries to connect to the HDFS NameNode. If the connection fails, &abbrev; will repeatedly attempt to connect again after the period set in <parameter>time-reopen()</parameter> expires.</para>
        </step>
        <step>
            <para>&abbrev; checks if the path to the logfile exists. If a directory does not exist &abbrev; automatically creates it. &abbrev; creates the destination file (using the filename set in the &abbrev; configuration file, with a UUID suffix to make it unique, for example, <filename>/usr/hadoop/logfile.txt.3dc1c59e-ab3b-4b71-9e81-93db477ed9d9</filename>) and writes the message into the file. After the file is created, &abbrev; will write all incoming messages into the <parameter>hdfs</parameter> destination.</para>
            <note>
                <para>When the <link linkend="option-hdfs-append-enabled"><parameter>hdfs-append-enabled()</parameter></link> option is set to <userinput>true</userinput>, &abbrev; will not assign a new UUID suffix to an existing file, because it is then possible to open a closed file and append data to that. </para>
            </note>
            <note>
                <xi:include href="../../shared/chunk/para-hdfs-flush.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            </note>
        </step>
        <step>
            <para>If the HDFS client returns an error, &abbrev; attempts to close the file, then opens a new file and repeats sending the message (trying to connect to HDFS and send the message), as set in the <parameter>retries()</parameter> parameter. If sending the message fails for <parameter>retries()</parameter> times, &abbrev; drops the message.</para>
        </step>
        <step>
            <para>The &abbrev; application closes the destination file in the following cases:</para>
            <itemizedlist>
                <listitem>
                    <para>&abbrev; is reloaded</para>
                </listitem>
                <listitem>
                    <para>&abbrev; is restarted</para>
                </listitem>
                <listitem>
                    <para>The HDFS client returns an error.</para>
                </listitem>
            </itemizedlist>
        </step>
        <step>
            <para>If the file is closed and you have set an archive directory, &abbrev; moves the file to this directory. If &abbrev; cannot move the file for some reason (for example, &abbrev; cannot connect to the HDFS NameNode), the file remains at its original location, &abbrev; will not try to move it again.</para>
        </step>
    </procedure>
    <procedure xml:id="destination-hdfs-maprfs">
        <title>Storing messages with MapR-FS</title>
        <indexterm> <primary>MapR</primary> </indexterm>
        <indexterm> <primary>MapR-FS</primary> </indexterm>
        <indexterm> <primary>MapR File System</primary> </indexterm>
        <para>The &abbrev; application is also compatible with MapR File System (MapR-FS). MapR-FS provides better performance, reliability, efficiency, maintainability, and ease of use compared to the default Hadoop Distributed Files System (HDFS). To use MapR-FS with &abbrev;, complete the following steps:</para>
        <step>
            <para>Install MapR libraries. Instead of the official Apache HDFS libraries, MapR uses different libraries. The supported version is MapR 4.x.</para>
            <substeps>
                <step>
                    <para>Download the libraries from the <link xmlns:ns1="http://doc.mapr.com/display/MapR3/Maven+Repository+and+Artifacts+for+MapR">Maven Repository and Artifacts for MapR</link> or get it from <link xmlns:ns1="http://doc.mapr.com/display/MapR/Quick+Installation+Guide">an already existing MapR installation</link>.</para>
                </step>
                <step>
                    <para>Install MapR. If you do not know how to install MapR, follow the instructions on the <link xmlns:ns1="https://www.mapr.com/">MapR website</link>.</para>
                </step>
            </substeps>
        </step>
        <step>
            <para>In a default MapR installation, the required libraries are installed in the following path: <filename>/opt/mapr/lib</filename>.</para>
            <para>Enter the path where MapR was installed in the <parameter>class-path</parameter> option of the <parameter>hdfs</parameter> destination, for example:</para>
            <synopsis>class-path("/opt/mapr/lib/")</synopsis>
            <para>If the libraries were downloaded from the Maven Repository, the following additional libraries will be requiered. Note that the version numbers in the filenames can be different in the various Hadoop releases:<filename>commons-collections-3.2.1.jar</filename>, <filename>commons-logging-1.1.3.jar</filename>, <filename>hadoop-auth-2.5.1.jar</filename>, <filename>log4j-1.2.15.jar</filename>, <filename>slf4j-api-1.7.5.jar</filename>, <filename>commons-configuration-1.6.jar</filename>, <filename>guava-13.0.1.jar</filename>, <filename>hadoop-common-2.5.1.jar</filename>, <filename>maprfs-4.0.2-mapr.jar</filename>, <filename>slf4j-log4j12-1.7.5.jar</filename>, <filename>commons-lang-2.5.jar</filename>, <filename>hadoop-0.20.2-dev-core.jar</filename>, <filename>json-20080701.jar</filename>, <filename>protobuf-java-2.5.0.jar</filename>, <filename>zookeeper-3.4.5-mapr-1406.jar</filename>.</para>
        </step>
        <step>
            <para>Configure the <parameter>hdfs</parameter> destination in &abbrev;.</para>
            <example xml:id="example-destination-hdfs-mapr">
                <title>Storing logfiles with MapR-FS</title>
                <para>The following example defines an <parameter>hdfs</parameter> destination for MapR-FS using only the required parameters.</para>
                <synopsis>@module mod-java
@include "scl.conf"

destination d_mapr {
    hdfs(
        client-lib-dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:/opt/mapr/lib/")
        hdfs-uri("maprfs://10.140.32.80")
        hdfs-file("/user/log/logfile.txt")
    );
};
</synopsis>
            </example>
        </step>
    </procedure>
    <section xml:id="destination-hdfs-kerberos-authentication">
        <title>Kerberos authentication with syslog-ng hdfs() destination</title>
        <para>Version <phrase condition="ose">3.10</phrase><phrase condition="pe">7.0.3</phrase> and later supports Kerberos authentication to authenticate the connection to your Hadoop cluster. &abbrev; assumes that you already have a Hadoop and Kerberos infrastructure.</para>
        <note>
            <para>If you configure Kerberos authentication for a <parameter>hdfs()</parameter> destination, it affects all <parameter>hdfs()</parameter> destinations. Kerberos and non-Kerberos <parameter>hdfs()</parameter> destinations cannot be mixed in a &abbrev; configuration. This means that if one <parameter>hdfs()</parameter> destination uses Kerberos authentication, you have to configure all other <parameter>hdfs()</parameter> destinations to use Kerberos authentication too.</para>
            <para>Failing to do so results in non-Kerberos <parameter>hdfs()</parameter> destinations being unable to authenticate to the HDFS server.</para>
        </note>
        <note>
            <para>If you want to configure your <parameter>hdfs()</parameter> destination to stop using Kerberos authentication, namely, to remove Kerberos-related options from the <parameter>hdfs()</parameter> destination configuration, make sure to restart &abbrev; for the changes to take effect.</para>
        </note>
        <formalpara>
            <title>Prerequisites:</title>
            <para/>
        </formalpara>
        <itemizedlist>
            <listitem>
                <para>You have configured your Hadoop infrastructure to use Kerberos authentication.</para>
            </listitem>
            <listitem>
                <para>You have a keytab file and a principal for the host running &abbrev;. For details, see the <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="http://web.mit.edu/Kerberos/krb5-1.5/krb5-1.5.4/doc/krb5-install/The-Keytab-File.html">Kerberos documentation</link>.</para>
            </listitem>
            <listitem>
                <para>You have installed and configured the Kerberos client packages on the host running &abbrev;. (That is, Kerberos authentication works for the host, for example, from the command line using the <command>kinit user@REALM -k -t &lt;keytab_file&gt;</command> command.)</para>
            </listitem>
        </itemizedlist>
        <xi:include href="../../shared/chunk/synopsis-hdfs-kerberos-example.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    </section>
    <section xml:id="reference-destination-hdfs">
        <title>HDFS destination options</title>
        <indexterm> <primary>destination drivers</primary> <secondary><parameter>java()</parameter> driver</secondary> </indexterm>
        <indexterm> <primary>destination drivers</primary> <secondary><parameter>hdfs</parameter></secondary> </indexterm>
        <para>The <parameter>hdfs</parameter> destination stores the log messages in files on the Hadoop Distributed File System (HDFS). The <parameter>hdfs</parameter> destination has the following options.</para>
        <para>The following options are required: <parameter>hdfs-file()</parameter>, <parameter>hdfs-uri()</parameter>. Note that to use <parameter>hdfs</parameter>, you must add the following lines to the beginning of your &abbrev; configuration:</para>
        <synopsis>@module mod-java
@include "scl.conf"</synopsis>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-java-class-path.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            <para>For the <parameter>hdfs</parameter> destination, include the path to the directory where you copied the required libraries (see <xref linkend="destination-hdfs-prerequisites"/>), for example, <userinput>client-lib-dir("/opt/syslog-ng/lib/syslog-ng/java-modules/*.jar:/opt/hadoop/libs/*.jar")</userinput>.</para>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-diskbuffer.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-frac-digits.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="option-hdfs-append-enabled">
            <xi:include href="../../shared/chunk/option-destination-hdfs-append-enabled.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-archive-dir">
            <title>hdfs-archive-dir()</title>
            <indexterm type="parameter"> <primary>hdfs-archive-dir()</primary> <secondary>hdfs</secondary> </indexterm>
            <indexterm type="parameter"> <primary>hdfs</primary> <secondary>hdfs-archive-dir</secondary> </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path where &abbrev; will move the closed log files. If &abbrev; cannot move the file for some reason (for example, &abbrev; cannot connect to the HDFS NameNode), the file remains at its original location. For example, <userinput>hdfs-archive-dir("/usr/hdfs/archive/")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-file">
            <title>hdfs-file()</title>
            <indexterm type="parameter"> <primary>hdfs-file()</primary> <secondary>hdfs</secondary> </indexterm>
            <indexterm type="parameter"> <primary>hdfs</primary> <secondary>hdfs-file</secondary> </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path and name of the log file. For example, <userinput>hdfs-file("/usr/hdfs/mylogfile.txt")</userinput>. &abbrev; checks if the path to the logfile exists. If a directory does not exist &abbrev; automatically creates it.</para>
            <para><parameter>hdfs-file()</parameter> supports the usage of macros. This means that &abbrev; can create files on HDFS dynamically, using macros in the file (or directory) name.</para>
            <note>
                <para>When a filename resolved from the macros contains a character that HDFS does not support, &abbrev; will not be able to create the file. Make sure that you use macros that do not contain unsupported characters.</para>
            </note>
            <example>
                <title>Using macros in filenames</title>
                <para>In the following example, a <filename>/var/testdb_working_dir/$DAY-$HOUR.txt</filename> file will be created (with a UUID suffix):</para>
                <synopsis>destination d_hdfs_9bf3ff45341643c69bf46bfff940372a {
    hdfs(client-lib-dir(/hdfs-libs)
 hdfs-uri("hdfs://hdp2.syslog-ng.balabit:8020")
 hdfs-file("/var/testdb_working_dir/$DAY-$HOUR.txt"));
};</synopsis>
                <para>As an example, if it is the 31st day of the month and it is 12 o'clock, then the name of the file will be <filename>31-12.txt</filename>.</para>
            </example>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-max-filename-length">
            <title>hdfs-max-filename-length()</title>
            <indexterm type="parameter"> <primary>hdfs-max-filename-length()</primary> <secondary>hdfs</secondary> </indexterm>
            <indexterm type="parameter"> <primary>hdfs</primary> <secondary>hdfs-max-filename-length</secondary> </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>number</entry>
                        </row>
                        <row>
                            <entry>Default: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>255</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The maximum length of the filename. This filename (including the UUID that &abbrev; appends to it) cannot be longer than what the file system permits. If the filename is longer than the value of <parameter>hdfs-max-filename-length</parameter>, &abbrev; will automatically truncate the filename. For example, <userinput>hdfs-max-filename-length("255")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-resources">
            <title>hdfs-resources()</title>
            <indexterm type="parameter"> <primary>hdfs-resources()</primary> <secondary>hdfs</secondary> </indexterm>
            <indexterm type="parameter"> <primary>hdfs</primary> <secondary>hdfs-resources</secondary> </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The list of Hadoop resources to load, separated by semicolons. For example, <userinput>hdfs-resources("/home/user/hadoop/core-site.xml;/home/user/hadoop/hdfs-site.xml")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-uri">
            <title>hdfs-uri()</title>
            <indexterm type="parameter"> <primary>hdfs-uri()</primary> <secondary>hdfs</secondary> </indexterm>
            <indexterm type="parameter"> <primary>hdfs</primary> <secondary>hdfs-uri</secondary> </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The URI of the HDFS NameNode is in <userinput>hdfs://IPaddress:port</userinput> or <userinput>hdfs://hostname:port</userinput> format. When using MapR-FS, the URI of the MapR-FS NameNode is in <userinput>maprfs://IPaddress</userinput> or <userinput>maprfs://hostname</userinput> format, for example: <userinput>maprfs://10.140.32.80</userinput>. The IP address of the node can be IPv4 or IPv6. For example, <userinput>hdfs-uri("hdfs://10.140.32.80:8020")</userinput>. The IPv6 address must be enclosed in square brackets (<emphasis>[]</emphasis>) as specified by RFC 2732, for example, <userinput>hdfs-uri("hdfs://[FEDC:BA98:7654:3210:FEDC:BA98:7654:3210]:8020")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-destination-jvm-options">
            <xi:include href="../../shared/chunk/option-destination-jvm-options.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="hdfs-option-kerberos-keytab-file">
            <title>kerberos-keytab-file()</title>
            <indexterm type="parameter"> <primary>kerberos-keytab-file()</primary> <secondary>hdfs</secondary> </indexterm>
            <indexterm type="parameter"> <primary>hdfs</primary> <secondary>kerberos-keytab-file</secondary> </indexterm>
            <indexterm> <primary>kerberos</primary> <secondary>hdfs</secondary> </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path to the Kerberos keytab file that you received from your Kerberos administrator. For example, <userinput>kerberos-keytab-file("/opt/syslog-ng/etc/hdfs.headless.keytab")</userinput>. This option is needed only if you want to authenticate using Kerberos in Hadoop. You also have to set the <link linkend="hdfs-option-kerberos-principal"><parameter>hdfs-option-kerberos-principal()</parameter></link> option. For details on the using Kerberos authentication with the <parameter>hdfs()</parameter> destination, see <xref linkend="destination-hdfs-kerberos-authentication"/>.</para>
            <xi:include href="../../shared/chunk/synopsis-hdfs-kerberos-example.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            <para>Available in &abbrev; version <phrase condition="ose">3.10</phrase><phrase condition="pe">7.0.3</phrase> and later.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-kerberos-principal">
            <title>kerberos-principal()</title>
            <indexterm type="parameter"> <primary>kerberos-principal()</primary> <secondary>hdfs</secondary> </indexterm>
            <indexterm type="parameter"> <primary>hdfs</primary> <secondary>kerberos-principal</secondary> </indexterm>
            <indexterm> <primary>kerberos</primary> <secondary>hdfs</secondary> </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default: <?dbhtml bgcolor="#D4D6EB" ?>
                                <?dbfo bgcolor="#D4D6EB" ?> </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The Kerberos principal you want to authenticate with. For example, <userinput>kerberos-principal("hdfs-user@MYREALM")</userinput>. This option is needed only if you want to authenticate using Kerberos in Hadoop. You also have to set the <link linkend="hdfs-option-kerberos-keytab-file"><parameter>hdfs-option-kerberos-keytab-file()</parameter></link> option. For details on the using Kerberos authentication with the <parameter>hdfs()</parameter> destination, see <xref linkend="destination-hdfs-kerberos-authentication"/>.</para>
            <xi:include href="../../shared/chunk/synopsis-hdfs-kerberos-example.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            <para>Available in &abbrev; version <phrase condition="ose">3.10</phrase><phrase condition="pe">7.0.3</phrase> and later.</para>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-log-fifo-size.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-on-error.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-retries.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-template.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-throttle.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-timezone.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../shared/chunk/option-destination-ts-format.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
    </section>
</section>
