<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE section
 [  <!ENTITY % entities SYSTEM "../../common/syslog-ng-entities.ent">
 %entities;]>
<section xml:id="configuring-destinations-hdfs" xmlns="http://docbook.org/ns/docbook" version="5.0">
    <title>Storing messages on the Hadoop Distributed File System (HDFS)</title>
    <indexterm>
        <primary>destination drivers</primary>
        <secondary><parameter>java()</parameter> driver</secondary>
    </indexterm>
    <indexterm>
        <primary>destination drivers</primary>
        <secondary><parameter>hdfs</parameter></secondary>
    </indexterm>
    <para>Starting with version <phrase condition="pe">5.3</phrase><phrase condition="ose">3.7</phrase>, &abbrev; can send plain-text log files to the <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="http://hadoop.apache.org/">Hadoop Distributed File System (HDFS)</link>, allowing you to store your log data on a distributed, scalable file system. This is especially useful if you have huge amount of log messages that would be difficult to store otherwise, or if you want to process your messages using Hadoop tools (for example, Apache Pig).</para>
    <xi:include href="../../common/wnt/note-server-mode-only.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    <para>Note the following limitations when using the &abbrev; <parameter>hdfs</parameter> destination:</para>
    <itemizedlist>
        <xi:include href="../../common/chunk/listitem-java-supported-platforms.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        <listitem>
            <para>Since &abbrev; uses the official Java HDFS client, the <parameter>hdfs</parameter> destination has significant memory usage (about 400MB).</para>
        </listitem>
        <listitem>
            <para>The &abbrev; application always creates a new file if the previous has been closed. Appending data to existing files is not supported.</para>
        </listitem>
        <listitem>
            <xi:include href="../../common/chunk/para-hdfs-filename.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </listitem>
        <listitem>
            <xi:include href="../../common/chunk/para-hdfs-flush.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </listitem>
        <xi:include href="../../common/chunk/listitem-java-internal-messages.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
    </itemizedlist>
    <note condition="pe">
        <para>The <parameter>hdfs</parameter> destination has been tested with Hortonworks Data Platform.</para>
    </note>
    <formalpara>
        <title>Declaration:</title>
        <para/>
    </formalpara>
    <synopsis>@module mod-java
@include "scl.conf"

hdfs(
    client_lib_dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:&lt;path-to-preinstalled-hadoop-libraries&gt;")
    hdfs_uri("hdfs://NameNode:8020")
    hdfs_file("&lt;path-to-logfile&gt;")
);</synopsis>
    <example xml:id="example-destination-hdfs">
        <title>Storing logfiles on HDFS</title>
        <para>The following example defines an <parameter>hdfs</parameter> destination using only the required parameters.</para>
        <synopsis>@module mod-java
@include "scl.conf"

destination d_hdfs {
    hdfs(
        client_lib_dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:/opt/hadoop/libs")
        hdfs_uri("hdfs://10.140.32.80:8020")
        hdfs_file("/user/log/logfile.txt")
    );
};
</synopsis>
    </example>
    <itemizedlist>
        <listitem>
            <para>To install the software required for the <parameter>hdfs</parameter> destination, see <xref linkend="destination-hdfs-prerequisites"/>.</para>
        </listitem>
        <listitem>
            <para>For details on how the <parameter>hdfs</parameter> destination works, see <xref linkend="destination-hdfs-interaction"/>.</para>
        </listitem>
        <listitem>
            <para>For details on using MapR-FS, see <xref linkend="destination-hdfs-maprfs"/>.</para>
        </listitem>
        <listitem>
            <para>For the list of options, see <xref linkend="reference-destination-hdfs"/>.</para>
        </listitem>
    </itemizedlist>
    <procedure xml:id="destination-hdfs-prerequisites">
        <title>Prerequisites</title>
        <para>To send messages from &abbrev; to HDFS, complete the following steps.</para>
        <formalpara>
            <title>Steps:</title>
            <para/>
        </formalpara>
        <step>
            <para>If you want to use the Java-based modules of &abbrev; (for example, the Elasticsearch, HDFS, or Kafka destinations), download and install the Java Runtime Environment (JRE), 1.7 (or newer). </para>
            <para><phrase condition="pe">The Java-based modules of &abbrev; are tested and supported when using the Oracle implementation of Java. Other implementations are untested and unsupported, they may or may not work as expected.</phrase><phrase condition="ose">You can use OpenJDK or Oracle JDK, other implementations are not tested.</phrase></para>
        </step>
        <step>
            <para>Download the Hadoop Distributed File System (HDFS) libraries (version 2.x) from <link xmlns:ns1="http://www.w3.org/1999/xlink" ns1:href="http://hadoop.apache.org/releases.html">http://hadoop.apache.org/releases.html</link>.</para>
        </step>
        <step>
            <para>Extract the HDFS libraries into a temporary directory, then collect the various <filename>.jar</filename> files into a single directory (for example, <filename>/opt/hadoop/lib/</filename>) where &abbrev; can access them. You must specify this directory in the &abbrev; configuration file. The files are located in the various <filename>lib</filename> directories under the <filename>share/</filename> directory of the Hadoop release package. (For example, in Hadoop 2.7, required files are <filename>common/hadoop-common-2.7.0.jar</filename>, <filename>common/libs/*.jar</filename>, <filename>hdfs/hadoop-hdfs-2.7.0.jar</filename>, <filename>hdfs/lib/*</filename>, but this may change between Hadoop releases, so it is easier to copy every <filename>.jar</filename> file into a single directory.</para>
        </step>
<!-- The java home must be given at installation time for the PE installer. FIXME: include it in the PE installer description!  -->
    </procedure>
    <procedure xml:id="destination-hdfs-interaction">
        <title>How &abbrev; interacts with HDFS</title>
        <para>The &abbrev; application sends the log messages to the official HDFS client library, which forwards the data to the HDFS nodes. The way how &abbrev; interacts with HDFS is described in the following steps.</para>
        <step>
            <para>After &abbrev; is started and the first message arrives to the <parameter>hdfs</parameter> destination, the <parameter>hdfs</parameter> destination tries to connect to the HDFS NameNode. If the connection fails, &abbrev; will repeatedly attempt to connect again after the period set in <parameter>time-reopen()</parameter> expires.</para>
        </step>
        <step>
            <para>&abbrev; checks if the path to the logfile exists. If a directory does not exist &abbrev; automatically creates it. &abbrev; creates the destination file (using the filename set in the &abbrev; configuration file, with a UUID suffix to make it unique, for example, <filename>/usr/hadoop/logfile.txt.3dc1c59e-ab3b-4b71-9e81-93db477ed9d9</filename>) and writes the message into the file. After the file is created, &abbrev; will write all incoming messages into the <parameter>hdfs</parameter> destination.</para>
            <note>
                <xi:include href="../../common/chunk/para-hdfs-flush.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            </note>
        </step>
        <step>
            <para>If the HDFS client returns an error, &abbrev; attempts to close the file, then opens a new file and repeats sending the message (trying to connect to HDFS and send the message), as set in the <parameter>retries()</parameter> parameter. If sending the message fails for <parameter>retries()</parameter> times, &abbrev; drops the message.</para>
        </step>
        <step>
            <para>The &abbrev; application closes the destination file in the following cases:</para>
            <itemizedlist>
                <listitem>
                    <para>&abbrev; is reloaded</para>
                </listitem>
                <listitem>
                    <para>&abbrev; is restarted</para>
                </listitem>
                <listitem>
                    <para>The HDFS client returns an error.</para>
                </listitem>
            </itemizedlist>
        </step>
        <step>
            <para>If the file is closed and you have set an archive directory, &abbrev; moves the file to this directory. If &abbrev; cannot move the file for some reason (for example, &abbrev; cannot connect to the HDFS NameNode), the file remains at its original location, &abbrev; will not try to move it again.</para>
        </step>
    </procedure>
    <procedure xml:id="destination-hdfs-maprfs">
        <title>Storing messages with MapR-FS</title>
        <indexterm>
            <primary>MapR</primary>
        </indexterm>
        <indexterm>
            <primary>MapR-FS</primary>
        </indexterm>
        <indexterm>
            <primary>MapR File System</primary>
        </indexterm>
        <para>The &abbrev; application is also compatible with MapR File System (MapR-FS), starting from version 5.4, &product; is <emphasis>MapR</emphasis> certified. MapR-FS provides better performance, reliability, efficiency, maintainability, and ease of use compared to the default Hadoop Distributed Files System (HDFS). To use MapR-FS with &abbrev;, complete the following steps:</para>
        <step>
            <para>Install MapR libraries. Instead of the official Apache HDFS libraries, MapR uses different libraries. The supported version is MapR 4.x.</para>
            <substeps>
                <step>
                    <para>Download the libraries from the <link xmlns:ns1="http://doc.mapr.com/display/MapR3/Maven+Repository+and+Artifacts+for+MapR">Maven Repository and Artifacts for MapR</link> or get it from <link xmlns:ns1="http://doc.mapr.com/display/MapR/Quick+Installation+Guide">an already existing MapR installation</link>.</para>
                </step>
                <step>
                    <para>Install MapR. If you do not know how to install MapR, follow the instructions on the <link xmlns:ns1="https://www.mapr.com/">MapR website</link>.</para>
                </step>
            </substeps>
        </step>
        <step>
            <para>In a default MapR installation, the required libraries are installed in the following path: <filename>/opt/mapr/lib</filename>.</para>
            <para>Enter the path where MapR was installed in the <parameter>class-path</parameter> option of the <parameter>hdfs</parameter> destination, for example:</para>
            <synopsis>class_path("/opt/mapr/lib/")</synopsis>
            <para>If the libraries were downloaded from the Maven Repository, the following additional libraries will be requiered. Note that the version numbers in the filenames can be different in the various Hadoop releases:<filename>commons-collections-3.2.1.jar</filename>, <filename>commons-logging-1.1.3.jar</filename>, <filename>hadoop-auth-2.5.1.jar</filename>, <filename>log4j-1.2.15.jar</filename>, <filename>slf4j-api-1.7.5.jar</filename>, <filename>commons-configuration-1.6.jar</filename>, <filename>guava-13.0.1.jar</filename>, <filename>hadoop-common-2.5.1.jar</filename>, <filename>maprfs-4.0.2-mapr.jar</filename>, <filename>slf4j-log4j12-1.7.5.jar</filename>, <filename>commons-lang-2.5.jar</filename>, <filename>hadoop-0.20.2-dev-core.jar</filename>, <filename>json-20080701.jar</filename>, <filename>protobuf-java-2.5.0.jar</filename>, <filename>zookeeper-3.4.5-mapr-1406.jar</filename>.</para>
        </step>
        <step>
            <para>Configure the <parameter>hdfs</parameter> destination in &abbrev;.</para>
            <example xml:id="example-destination-hdfs-mapr">
                <title>Storing logfiles with MapR-FS</title>
                <para>The following example defines an <parameter>hdfs</parameter> destination for MapR-FS using only the required parameters.</para>
                <synopsis>@module mod-java
@include "scl.conf"

destination d_mapr {
    hdfs(
        client_lib_dir("/opt/syslog-ng/lib/syslog-ng/java-modules/:/opt/mapr/lib/")
        hdfs_uri(maprfs://10.140.32.80")
        hdfs_file("/user/log/logfile.txt")
    );
};
</synopsis>
            </example>
        </step>
    </procedure>
    <section xml:id="reference-destination-hdfs">
        <title>HDSF destination options</title>
        <indexterm>
            <primary>destination drivers</primary>
            <secondary><parameter>java()</parameter> driver</secondary>
        </indexterm>
        <indexterm>
            <primary>destination drivers</primary>
            <secondary><parameter>hdfs</parameter></secondary>
        </indexterm>
        <para>The <parameter>hdfs</parameter> destination stores the log messages in files on the Hadoop Distributed File System (HDFS). The <parameter>hdfs</parameter> destination has the following options.</para>
        <para>The following options are required: <parameter>hdfs_file()</parameter>, <parameter>hdfs_uri()</parameter>. Note that to use <parameter>hdfs</parameter>, you must add the following lines to the beginning of your &abbrev; configuration:</para>
        <synopsis>@module mod-java
@include "scl.conf"</synopsis>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-java-class-path.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
            <para><emphasis role="bold">Description:</emphasis> Include the path to the directory where you copied the required libraries (see <xref linkend="destination-hdfs-prerequisites"/>), for example, <userinput>client_lib_dir(/user/share/hadoop/lib)</userinput>.</para>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-diskbuffer.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-frac-digits.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs_archive_dir">
            <title>hdfs_archive_dir()</title>
            <indexterm type="parameter">
                <primary>hdfs_archive_dir()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs_archive_dir</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path where &abbrev; will move the closed log files. If &abbrev; cannot move the file for some reason (for example, &abbrev; cannot connect to the HDFS NameNode), the file remains at its original location. For example, <userinput>hdfs_archive_dir("/usr/hdfs/archive/")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs_file">
            <title>hdfs_file()</title>
            <indexterm type="parameter">
                <primary>hdfs_file()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs_file</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The path and name of the log file. For example, <userinput>hdfs_file("/usr/hdfs/mylogfile.txt")</userinput>. &abbrev; checks if the path to the logfile exists. If a directory does not exist &abbrev; automatically creates it.</para>
            <xi:include href="../../common/chunk/para-hdfs-filename.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-max-filename-length">
            <title>hdfs_max_filename_length()</title>
            <indexterm type="parameter">
                <primary>hdfs_max_filename_length()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs_max_filename_length</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>number</entry>
                        </row>
                        <row>
                            <entry>Default:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>255</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The maximum length of the filename. This filename (including the UUID that &abbrev; appends to it) cannot be longer than what the file system permits. If the filename is longer than the value of <parameter>hdfs_max_filename_length</parameter>, &abbrev; will automatically truncate the filename. For example, <userinput>hdfs_max_filename_length("255")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs-resources">
            <title>hdfs_resources()</title>
            <indexterm type="parameter">
                <primary>hdfs_resources()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs_resources</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The list of Hadoop resources to load, separated by semicolons. For example, <userinput>hdfs_resources("/home/user/hadoop/core-site.xml;/home/user/hadoop/hdfs-site.xml")</userinput>.</para>
        </simplesect>
        <simplesect xml:id="hdfs-option-hdfs_uri">
            <title>hdfs_uri()</title>
            <indexterm type="parameter">
                <primary>hdfs_uri()</primary>
                <secondary>hdfs</secondary>
            </indexterm>
            <indexterm type="parameter">
                <primary>hdfs</primary>
                <secondary>hdfs_uri</secondary>
            </indexterm>
            <informaltable frame="topbot" colsep="0" rowsep="0">
                <tgroup cols="2">
                    <colspec colnum="1" colwidth="40pt"/>
                    <tbody>
                        <row>
                            <entry>Type:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>string</entry>
                        </row>
                        <row>
                            <entry>Default:
                                <?dbhtml bgcolor="#D4D6EB" ?>
<?dbfo bgcolor="#D4D6EB" ?>
                            </entry>
                            <entry>N/A</entry>
                        </row>
                    </tbody>
                </tgroup>
            </informaltable>
            <para><emphasis role="bold">Description:</emphasis> The URI of the HDFS NameNode is in <userinput>hdfs://IPaddress:port</userinput> or <userinput>hdfs://hostname:port</userinput> format. When using MapR-FS, the URI of the MapR-FS NameNode is in <userinput>maprfs://IPaddress</userinput> or <userinput>maprfs://hostname</userinput> format, for example: <userinput>maprfs://10.140.32.80</userinput>. The IP address of the node can be IPv4 or IPv6. For example, <userinput>hdfs_uri("hdfs://10.140.32.80:8020")</userinput>. The IPv6 address must be enclosed in square brackets (<emphasis>[]</emphasis>) as specified by RFC 2732, for example, <userinput>hdfs_uri("hdfs://[FEDC:BA98:7654:3210:FEDC:BA98:7654:3210]:8020")</userinput>.</para>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-log-fifo-size.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-on-error.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-retries.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-template.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-throttle.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-timezone.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
        <simplesect>
            <xi:include href="../../common/chunk/option-destination-ts-format.xml" xmlns:xi="http://www.w3.org/2001/XInclude"/>
        </simplesect>
    </section>
</section>
